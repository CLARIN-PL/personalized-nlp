{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import evaluate\n",
    "import datasets\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    EvalPrediction,\n",
    "    TrainingArguments,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer\n",
    ")\n",
    "from tqdm import tqdm\n",
    "from transformers.adapters import  AdapterTrainer, AutoAdapterModel, AdapterConfig\n",
    "\n",
    "from personalized_nlp.models.adapters.heads import RegressionHead\n",
    "from personalized_nlp.datasets.doccano.doccano import DoccanoDataModule\n",
    "from personalized_nlp.utils import seed_everything\n",
    "from settings import STORAGE_DIR\n",
    "\n",
    "\n",
    "BATCHSIZE = 16\n",
    "\n",
    "OUTPUT_DIR = STORAGE_DIR / \"adapter_finetuned\" / 'results'\n",
    "PRETRAINED_DIR = STORAGE_DIR / \"adapter_finetuned\" / 'pretrained_models'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(PRETRAINED_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):  # type: ignore\n",
    "    return tokenizer(\n",
    "        examples[\"text\"], padding=\"max_length\", truncation=True, max_length=128\n",
    "    )\n",
    "\n",
    "def compute_metrics(prediction: EvalPrediction) -> dict:\n",
    "    labels = prediction.label_ids\n",
    "    predictions = prediction.predictions\n",
    "\n",
    "    mse = evaluate.load(\"mse\")\n",
    "    mse_val = mse.compute(references=labels.flatten(), predictions=predictions.flatten())\n",
    "\n",
    "    return {\n",
    "        \"mse\": mse_val,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fold in range(5):\n",
    "    seed_everything()\n",
    "\n",
    "    experiment_file_path = PRETRAINED_DIR / f\"fold_{fold}.pkl\"\n",
    "\n",
    "    dm = DoccanoDataModule(regression=True, \n",
    "                        stratify_folds_by='texts', \n",
    "                        min_annotations_per_text=20, \n",
    "                        use_cuda=False, \n",
    "                        folds_num=5,\n",
    "                        test_fold=fold,\n",
    "                        empty_annotations_strategy=\"drop\", \n",
    "                        randomize_questionnaries=False)\n",
    "\n",
    "    data = dm.annotations.merge(dm.data)\n",
    "\n",
    "    full_data = data.loc[:, ['text', 'split']]\n",
    "    vals_arr = data[dm.annotation_columns].values\n",
    "    full_data['labels'] = [vals_arr[i] for i in range(vals_arr.shape[0])]\n",
    "\n",
    "    train_data = full_data.loc[full_data.split == 'train']\n",
    "    val_data = full_data.loc[full_data.split == 'val']\n",
    "    test_data = full_data.loc[full_data.split == 'test']\n",
    "\n",
    "    train_dataset = datasets.Dataset.from_pandas(train_data.loc[:, [\"text\", \"labels\"]])\n",
    "    train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "    val_dataset = datasets.Dataset.from_pandas(val_data.loc[:, [\"text\", \"labels\"]])\n",
    "    val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "    test_dataset = datasets.Dataset.from_pandas(test_data.loc[:, [\"text\"]])\n",
    "    test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        \"sentence-transformers/LaBSE\",\n",
    "       num_labels=vals_arr.shape[1],\n",
    "       problem_type='regression'\n",
    "    )\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/LaBSE\")\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=str(\"./tmp/test_trainer\"),\n",
    "        evaluation_strategy=\"epoch\",  # type: ignore\n",
    "        save_strategy=\"epoch\",  # type: ignore\n",
    "        per_device_train_batch_size=BATCHSIZE,\n",
    "        #report_to=\"mlflow\",  # type: ignore\n",
    "        num_train_epochs=6,\n",
    "        load_best_model_at_end=True,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=val_dataset,\n",
    "            compute_metrics=compute_metrics,\n",
    "        )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    preds = trainer.predict(test_dataset=test_dataset)  # type: ignore,\n",
    "\n",
    "    test_data['preds'] = [preds.predictions[i] for i in range(len(preds.predictions))]\n",
    "\n",
    "    test_data.to_pickle(experiment_file_path)\n",
    "    model.save_pretrained(PRETRAINED_DIR / f\"fold_{fold}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fold in range(5):\n",
    "    seed_everything()\n",
    "\n",
    "    dm = DoccanoDataModule(regression=True, \n",
    "                        stratify_folds_by='texts', \n",
    "                        min_annotations_per_text=20, \n",
    "                        use_cuda=False, \n",
    "                        folds_num=5,\n",
    "                        test_fold=fold,\n",
    "                        empty_annotations_strategy=\"drop\", \n",
    "                        randomize_questionnaries=False)\n",
    "\n",
    "    model = AutoAdapterModel.from_pretrained(\n",
    "        PRETRAINED_DIR / f\"fold_{fold}\",\n",
    "        num_labels=vals_arr.shape[1],\n",
    "        problem_type='regression'\n",
    "    )\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/LaBSE\")\n",
    "\n",
    "\n",
    "    for annotator_id in tqdm(range(40)):\n",
    "        experiment_file_path = OUTPUT_DIR / f\"annotator_{annotator_id}_fold_{fold}.pkl\"\n",
    "        # if os.path.exists(experiment_file_path):\n",
    "        #     continue\n",
    "\n",
    "        data = dm.annotations.loc[dm.annotations.user_id == annotator_id]\n",
    "\n",
    "        full_data = dm.data.merge(data).loc[:, ['text', 'split']]\n",
    "        vals_arr = dm.data.merge(data)[dm.annotation_columns].values\n",
    "        full_data['labels'] = [vals_arr[i] for i in range(vals_arr.shape[0])]\n",
    "\n",
    "        train_data = full_data.loc[full_data.split == 'train']\n",
    "        val_data = full_data.loc[full_data.split == 'val']\n",
    "        test_data = full_data.loc[full_data.split == 'test']\n",
    "\n",
    "        train_dataset = datasets.Dataset.from_pandas(train_data.loc[:, [\"text\", \"labels\"]])\n",
    "        train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "        val_dataset = datasets.Dataset.from_pandas(val_data.loc[:, [\"text\", \"labels\"]])\n",
    "        val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "        test_dataset = datasets.Dataset.from_pandas(test_data.loc[:, [\"text\"]])\n",
    "        test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "        task_name = f\"annotator_{annotator_id}\"\n",
    "\n",
    "        head = RegressionHead(model, task_name, num_labels=26)\n",
    "        model.add_prediction_head(head, overwrite_ok=True)\n",
    "\n",
    "        if task_name not in model.config.adapters:\n",
    "            adapter_config = AdapterConfig.load(\"pfeiffer\")\n",
    "            model.add_adapter(task_name, config=adapter_config)\n",
    "\n",
    "        model.train_adapter(task_name)\n",
    "        model.set_active_adapters(task_name)\n",
    "\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=str(\"./tmp/test_trainer\"),\n",
    "            evaluation_strategy=\"epoch\",  # type: ignore\n",
    "            save_strategy=\"epoch\",  # type: ignore\n",
    "            per_device_train_batch_size=BATCHSIZE,\n",
    "            #report_to=\"mlflow\",  # type: ignore\n",
    "            num_train_epochs=6,\n",
    "            load_best_model_at_end=True,\n",
    "        )\n",
    "\n",
    "        trainer = AdapterTrainer(\n",
    "                model=model,\n",
    "                args=training_args,\n",
    "                train_dataset=train_dataset,\n",
    "                eval_dataset=val_dataset,\n",
    "                compute_metrics=compute_metrics,\n",
    "            )\n",
    "\n",
    "        trainer.train()\n",
    "\n",
    "        preds = trainer.predict(test_dataset=test_dataset)  # type: ignore,\n",
    "\n",
    "        test_data['preds'] = [preds.predictions[i] for i in range(len(preds.predictions))]\n",
    "        test_data.to_pickle(experiment_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
